<!--
date: 2024-03-25T17:46:13
-->

[Video comparison](https://www.youtube.com/watch?v=f0TzppYWvUQ) 

![YouTube Preview](https://img.youtube.com/vi/f0TzppYWvUQ/mqdefault.jpg)

 of VSCode plugins for local code generation - [Continue](https://continue.dev/)  or [Twinny](https://marketplace.visualstudio.com/items?itemName=rjmacarthy.twinny) 

Options:
  ğŸ¤– Continue: everything is only through chat, no code autocompletion
  ğŸ¦™ Llama Coder: code autocompletion, no chat interface
  ğŸ” Cody from Sourcegraph: pricing model is unclear

  ğŸ‘¯â€â™‚ï¸ **Twinny**: a new project that ğŸ¤ combines the features of Llama Coder and Continue - chat and code autocompletion.

_For autocompletion, so that it does not hang, of course, you need to take a smaller "base" model (1-3B) and a more powerful computer.__For the chat to work, you also need to run the "instruct" model_.